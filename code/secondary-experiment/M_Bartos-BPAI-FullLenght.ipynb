{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Importing the Evaluation Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "file1 = '/Users/markbartos/Library/Mobile Documents/com~apple~CloudDocs/DRIVE/EDUCATION/VU_AI/YEAR3 PERIOD 5/BPAI/Full-Lenght Papers/PMC2949396.csv'\n",
    "file2 = '/Users/markbartos/Library/Mobile Documents/com~apple~CloudDocs/DRIVE/EDUCATION/VU_AI/YEAR3 PERIOD 5/BPAI/Full-Lenght Papers/PMC7799030.csv'\n",
    "file3 = '/Users/markbartos/Library/Mobile Documents/com~apple~CloudDocs/DRIVE/EDUCATION/VU_AI/YEAR3 PERIOD 5/BPAI/Full-Lenght Papers/PMC10707032.csv'\n",
    "\n",
    "# Read the CSV files into DataFrames\n",
    "df1 = pd.read_csv(file1)\n",
    "df2 = pd.read_csv(file2)\n",
    "df3 = pd.read_csv(file3)\n",
    "\n",
    "df1.columns = [\"token\", \"Annotations\"]\n",
    "df2.columns = [\"token\", \"Annotations\"]\n",
    "df3.columns = [\"token\", \"Annotations\"]\n",
    "\n",
    "df1['PMID'] = 'PMC2949396'\n",
    "df2['PMID'] = 'PMC7799030'\n",
    "df3['PMID'] = 'PMC10707032'\n",
    "\n",
    "# Display the first few rows of each DataFrame to understand their structure\n",
    "\n",
    "# Combine the dataframes into one\n",
    "combined_df = pd.concat([df1, df2, df3])\n",
    "\n",
    "# Convert to the desired format\n",
    "full_lenght_df = combined_df.groupby('PMID').apply(\n",
    "    lambda x: x[['token', 'Annotations']].to_dict('records')\n",
    ").reset_index(name='Annotations')\n",
    "\n",
    "# Display the combined and formatted DataFrame\n",
    "#print(full_lenght_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Importing the Train Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def read_data(directory):\n",
    "    data = []\n",
    "    for subdir, _, files in os.walk(os.path.join(directory, 'documents')):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.text'):\n",
    "                pmid = filename.split('.')[0]\n",
    "                with open(os.path.join(subdir, filename), 'r') as file:\n",
    "                    text = file.read().strip()\n",
    "                tokens_file = os.path.join(subdir, f\"{pmid}.tokens\")\n",
    "                with open(tokens_file, 'r') as file:\n",
    "                    tokens = file.read().strip().split()\n",
    "                data.append({'pmid': pmid, 'text': text, 'tokens': tokens})\n",
    "    return data\n",
    "\n",
    "def parse_pico(tokens, labels_p_file, labels_i_file, labels_o_file):\n",
    "    pico_annotations = []\n",
    "    with open(labels_p_file, 'r') as p_file, open(labels_i_file, 'r') as i_file, open(labels_o_file, 'r') as o_file:\n",
    "        labels_p = p_file.read().strip().split(',')\n",
    "        labels_i = i_file.read().strip().split(',')\n",
    "        labels_o = o_file.read().strip().split(',')\n",
    "\n",
    "    for token, p, i, o in zip(tokens, labels_p, labels_i, labels_o):\n",
    "        pico_annotations.append({\n",
    "            'token': token,\n",
    "            'P': int(p),\n",
    "            'I': int(i),\n",
    "            'O': int(o)\n",
    "        })\n",
    "    return pico_annotations\n",
    "\n",
    "data_directory = '/Users/markbartos/Library/Mobile Documents/com~apple~CloudDocs/DRIVE/EDUCATION/VU_AI/YEAR3 PERIOD 5/BPAI/Code/EBM-NLP-master/ebm_nlp_1_00'\n",
    "\n",
    "data = read_data(data_directory)\n",
    "\n",
    "sentence_data = []\n",
    "for entry in data:\n",
    "    pmid = entry['pmid']\n",
    "    Tokens = entry['tokens']\n",
    "    \n",
    "    labels_p_file = os.path.join(data_directory, 'annotations', 'aggregated', 'hierarchical_labels', 'participants', 'train', f\"{pmid}_AGGREGATED.ann\")\n",
    "    labels_i_file = os.path.join(data_directory, 'annotations', 'aggregated', 'hierarchical_labels', 'interventions', 'train', f\"{pmid}_AGGREGATED.ann\")\n",
    "    labels_o_file = os.path.join(data_directory, 'annotations', 'aggregated', 'hierarchical_labels', 'outcomes', 'train', f\"{pmid}_AGGREGATED.ann\")\n",
    "\n",
    "    if os.path.exists(labels_p_file) and os.path.exists(labels_i_file) and os.path.exists(labels_o_file):\n",
    "        annotations = parse_pico(Tokens, labels_p_file, labels_i_file, labels_o_file)\n",
    "        sentence_data.append({'PMID': pmid, 'Annotations': annotations})\n",
    "\n",
    "\n",
    "df = pd.DataFrame(sentence_data)\n",
    "#print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training the biLSTM model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating a df suitable for LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pico_map = {\n",
    "    'P': ['No label', 'Age', 'Sex', 'Sample size', 'Condition'],\n",
    "    'I': ['No label', 'Surgical', 'Physical', 'Drug', 'Educational', 'Psychological', 'Other', 'Control'],\n",
    "    'O': ['No label', 'Physical', 'Pain', 'Mortality', 'Adverse effects', 'Mental', 'Other']\n",
    "}\n",
    "\n",
    "def map_annotations(annotation):\n",
    "    mapped_p = pico_map['P'][annotation['P']] if annotation['P'] < len(pico_map['P']) else 'No label'\n",
    "    mapped_i = pico_map['I'][annotation['I']] if annotation['I'] < len(pico_map['I']) else 'No label'\n",
    "    mapped_o = pico_map['O'][annotation['O']] if annotation['O'] < len(pico_map['O']) else 'No label'\n",
    "\n",
    "    result = []\n",
    "    if mapped_p != 'No label':\n",
    "        result.append(f\"P-{mapped_p}\")\n",
    "    elif mapped_i != 'No label':\n",
    "        result.append(f\"I-{mapped_i}\")\n",
    "    elif mapped_o != 'No label':\n",
    "        result.append(f\"O-{mapped_o}\")\n",
    "\n",
    "    return ','.join(result) if result else '0'\n",
    "\n",
    "def create_lstm_df(df):\n",
    "    lstm_data = []\n",
    "    for index, row in df.iterrows():\n",
    "        tokens = [token['token'] for token in row['Annotations']]\n",
    "        annotations = [map_annotations(token) for token in row['Annotations']]\n",
    "        lstm_data.append({\"PMID\": pmid, \"Tokens\": tokens, \"Annotations\": annotations})\n",
    "    \n",
    "    return pd.DataFrame(lstm_data)\n",
    "\n",
    "\n",
    "LSTM_df = create_lstm_df(df)\n",
    "#print(LSTM_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_df2(df):\n",
    "    lstm_data = []\n",
    "    for index, row in df.iterrows():\n",
    "        tokens = [token['token'] for token in row['Annotations']]\n",
    "        annotations = [token['Annotations'] for token in row['Annotations']]\n",
    "        lstm_data.append({\"PMID\": row['PMID'], \"Tokens\": tokens, \"Annotations\": annotations})\n",
    "    \n",
    "    return pd.DataFrame(lstm_data)\n",
    "\n",
    "\n",
    "LSTM_test_df = create_lstm_df2(full_lenght_df)\n",
    "#print(LSTM_test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training the BiLSTM model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 500)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 500, 50)           1887500   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 500, 50)           0         \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 500, 200)          120800    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " time_distributed (TimeDist  (None, 500, 50)           10050     \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDi  (None, 500, 19)           969       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2019319 (7.70 MB)\n",
      "Trainable params: 2019319 (7.70 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "def prepare_data(df):\n",
    "    sentences = df['Tokens'].tolist()\n",
    "    labels = df['Annotations'].tolist()\n",
    "    return sentences, labels\n",
    "\n",
    "train_sentences, train_labels = prepare_data(LSTM_df)\n",
    "test_sentences, test_labels = prepare_data(LSTM_test_df)\n",
    "\n",
    "def build_model(max_len, n_words, n_tags):\n",
    "    input = Input(shape=(max_len,))\n",
    "    model = Embedding(input_dim=n_words, output_dim=50, input_length=max_len)(input)\n",
    "    model = Dropout(0.1)(model)\n",
    "    model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "    model = TimeDistributed(Dense(50, activation=\"relu\"))(model)\n",
    "    out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(model)\n",
    "    model = Model(input, out)\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "max_len = 500 # This is a cut-off value, to speed up training but it does increase loss\n",
    "n_words = len(set([token for sentence in train_sentences for token in sentence])) + 1\n",
    "n_tags = len(set([annotation for labels in train_labels for annotation in labels])) + 1\n",
    "\n",
    "model = build_model(max_len, n_words, n_tags)\n",
    "model.summary()\n",
    "\n",
    "def encode_data(sentences, labels, max_len, n_words, n_tags):\n",
    "    word2idx = {w: i for i, w in enumerate(set([token for sentence in sentences for token in sentence]), 1)}\n",
    "    label2idx = {l: i for i, l in enumerate(set([label for label_list in labels for label in label_list]), 1)}\n",
    "    \n",
    "    X = [[word2idx[token] for token in sentence] for sentence in sentences]\n",
    "    X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=n_words-1)\n",
    "    \n",
    "    y = [[label2idx[label] for label in label_list] for label_list in labels]\n",
    "    y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=n_tags-1)\n",
    "    \n",
    "    return X, np.array(y), word2idx, label2idx\n",
    "\n",
    "# Encode training data\n",
    "X_train, y_train, word2idx, label2idx = encode_data(train_sentences, train_labels, max_len, n_words, n_tags)\n",
    "X_test, y_test, _, _ = encode_data(test_sentences, test_labels, max_len, n_words, n_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "79/79 [==============================] - 36s 434ms/step - loss: 1.0145 - accuracy: 0.8627 - val_loss: 0.4986 - val_accuracy: 0.8980\n",
      "Epoch 2/10\n",
      "79/79 [==============================] - 37s 471ms/step - loss: 0.4930 - accuracy: 0.8961 - val_loss: 0.4772 - val_accuracy: 0.8980\n",
      "Epoch 3/10\n",
      "79/79 [==============================] - 37s 474ms/step - loss: 0.4540 - accuracy: 0.8961 - val_loss: 0.4376 - val_accuracy: 0.8981\n",
      "Epoch 4/10\n",
      "79/79 [==============================] - 37s 473ms/step - loss: 0.4081 - accuracy: 0.8979 - val_loss: 0.4267 - val_accuracy: 0.8986\n",
      "Epoch 5/10\n",
      "79/79 [==============================] - 37s 473ms/step - loss: 0.3784 - accuracy: 0.9012 - val_loss: 0.4135 - val_accuracy: 0.8990\n",
      "Epoch 6/10\n",
      "79/79 [==============================] - 40s 507ms/step - loss: 0.3409 - accuracy: 0.9076 - val_loss: 0.3916 - val_accuracy: 0.9003\n",
      "Epoch 7/10\n",
      "79/79 [==============================] - 39s 490ms/step - loss: 0.3060 - accuracy: 0.9150 - val_loss: 0.3882 - val_accuracy: 0.9038\n",
      "Epoch 8/10\n",
      "79/79 [==============================] - 39s 493ms/step - loss: 0.2722 - accuracy: 0.9213 - val_loss: 0.3791 - val_accuracy: 0.9007\n",
      "Epoch 9/10\n",
      "79/79 [==============================] - 39s 492ms/step - loss: 0.2450 - accuracy: 0.9270 - val_loss: 0.3897 - val_accuracy: 0.8996\n",
      "Epoch 10/10\n",
      "79/79 [==============================] - 39s 498ms/step - loss: 0.2234 - accuracy: 0.9318 - val_loss: 0.3966 - val_accuracy: 0.8996\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 1.3059 - accuracy: 0.6700\n",
      "Loss: 1.3059178590774536, Accuracy: 0.6700000166893005\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=32, epochs=10, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 71ms/step\n",
      "Label index to name mapping: {1: 'O-Other', 2: 'I-Educational', 3: 'I-Psychological', 4: 'P-Sample size', 5: 'O-Mental', 6: '0', 7: 'P-Condition', 8: 'I-Physical', 9: 'O-Mortality', 10: 'I-Control', 11: 'P-Age', 12: 'I-Drug', 13: 'P-Sex', 14: 'O-Pain', 15: 'O-Adverse effects', 16: 'O-Physical', 17: 'I-Surgical', 18: 'I-Other'}\n"
     ]
    }
   ],
   "source": [
    "# Predicting labels\n",
    "predictions = model.predict(X_test, verbose=1)\n",
    "\n",
    "# Converting predictions from encoded back to the original labels\n",
    "predicted_labels = np.argmax(predictions, axis=-1)\n",
    "idx2label = {i: label for label, i in label2idx.items()}\n",
    "print(\"Label index to name mapping:\", idx2label)\n",
    "\n",
    "def decode_predictions(predictions, sentences, idx2label):\n",
    "    decoded_predictions = []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        decoded_sentence = []\n",
    "        for j, token in enumerate(sentence):\n",
    "            if j < len(predictions[i]):\n",
    "                label_idx = predictions[i][j]\n",
    "                label = idx2label.get(label_idx, 'O')  # O if label not found\n",
    "                decoded_sentence.append((token, label))\n",
    "            else:\n",
    "                decoded_sentence.append((token, 'O'))  # O if label not found\n",
    "        decoded_predictions.append(decoded_sentence)\n",
    "    return decoded_predictions\n",
    "\n",
    "# Raise error if label not in range\n",
    "for label in predicted_labels.flatten():\n",
    "    if label not in idx2label:\n",
    "        print(f\"Warning: Predicted label index {label} not found in idx2label mapping.\")\n",
    "\n",
    "decoded_predictions = decode_predictions(predicted_labels, test_sentences, idx2label)\n",
    "\n",
    "def create_prediction_df(df, decoded_predictions):\n",
    "    prediction_data = []\n",
    "    for index, row in df.iterrows():\n",
    "        pmid = row[\"PMID\"]\n",
    "        tokens = row[\"Tokens\"]\n",
    "        annotations = [label for _, label in decoded_predictions[index]]\n",
    "        prediction_data.append({\"PMID\": pmid, \"Tokens\": tokens, \"Predictions\": annotations})\n",
    "    \n",
    "    return pd.DataFrame(prediction_data)\n",
    "\n",
    "# Implementing it back into a df\n",
    "prediction_df = create_prediction_df(LSTM_test_df, decoded_predictions)\n",
    "#print(prediction_df.head())\n",
    "\n",
    "# Ability to manually examine predictions\n",
    "for i in range(0): # Increase range to get started\n",
    "    print(f\"Sentence {i+1}:\")\n",
    "    for token, label in decoded_predictions[i]:\n",
    "        print(f\"{token}: {label}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Token-level Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "          O-Other       0.00      0.00      0.00         0\n",
      "    I-Educational       0.00      0.00      0.00        13\n",
      "  I-Psychological       0.00      0.00      0.00        46\n",
      "    P-Sample size       0.00      0.00      0.00         0\n",
      "         O-Mental       0.00      0.00      0.00         0\n",
      "                0       0.92      0.72      0.81      1394\n",
      "      P-Condition       0.00      0.00      0.00         0\n",
      "       I-Physical       0.00      0.00      0.00         0\n",
      "      O-Mortality       0.00      0.00      0.00        15\n",
      "        I-Control       0.00      0.00      0.00         0\n",
      "            P-Age       0.00      0.00      0.00         1\n",
      "           I-Drug       0.00      0.00      0.00         0\n",
      "            P-Sex       0.00      0.00      0.00         0\n",
      "           O-Pain       0.00      0.00      0.00        15\n",
      "O-Adverse effects       0.00      0.00      0.00         0\n",
      "       O-Physical       0.00      0.00      0.00        16\n",
      "       I-Surgical       0.00      0.00      0.00         0\n",
      "          I-Other       0.00      0.00      0.00         0\n",
      "\n",
      "        micro avg       0.67      0.67      0.67      1500\n",
      "        macro avg       0.05      0.04      0.04      1500\n",
      "     weighted avg       0.86      0.67      0.75      1500\n",
      "\n",
      "Accuracy: 0.6700\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "true_labels = y_test.flatten()\n",
    "pred_labels = predicted_labels.flatten()\n",
    "\n",
    "# Convert the labels back to their original names\n",
    "true_label_names = [idx2label[idx] for idx in true_labels]\n",
    "pred_label_names = [idx2label[idx] for idx in pred_labels]\n",
    "\n",
    "report = classification_report(true_label_names, pred_label_names, labels=list(idx2label.values()), zero_division=0, digits=2)\n",
    "print(report)\n",
    "\n",
    "token_accuracy = accuracy_score(true_label_names, pred_label_names)\n",
    "print(f\"Accuracy: {token_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setting up the GPT-4 model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating a suitable df for GPT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gpt_df(df):\n",
    "    gpt_data = []\n",
    "    for index, row in df.iterrows():\n",
    "        sentence = '•'.join([token['token'] for token in row['Annotations']])\n",
    "        annotation = '•'.join([token[\"Annotations\"] for token in row['Annotations']])\n",
    "        gpt_data.append({\"sentence\": sentence, \"annotation\": annotation})\n",
    "    \n",
    "    return pd.DataFrame(gpt_data)\n",
    "\n",
    "GPT_df = create_gpt_df(full_lenght_df)\n",
    "#print(GPT_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Predicting with GPT-4o**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "openai.api_key = \"sk-proj-oiuxDclszqeK2nXu4n9CT3BlbkFJttrqrKBM9FCiXJjJPHvp\"\n",
    "chunk_size = 75\n",
    "\n",
    "\n",
    "def get_annotations(sentence):\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a PICO annotator tasked with labeling tokens. For every chunk of 75 tokens separated by bullets (•), return 75 PICO annotations. Use one of the following options: 0 for none, P-Age, P-Sex, P-Sample size, P-Condition, I-Surgical, I-Physical, I-Drug, I-Educational, I-Psychological, I-Other, I-Control, O-Physical, O-Pain, O-Mortality, O-Adverse effects, O-Mental, O-Other. It is crucial that you return exactly 75 labels for each chunk, ensuring that the number of labels matches the number of tokens exactly.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Randomized•,•double-blind•,•placebo-controlled•trial•of•oral•sirolimus•for•restenosis•prevention•in•patients•with•in-stent•restenosis•:•the•Oral•Sirolimus•to•Inhibit•Recurrent•In-stent•Stenosis•(•OSIRIS•)•trial•.•BACKGROUND•Despite•recent•advances•in•interventional•cardiology•,•including•the•introduction•of•drug-eluting•stents•for•de•novo•coronary•lesions•,•the•treatment•of•in-stent•restenosis•(•ISR•)•remains•a•challenging•clinical•issue•.•Given•the•efficacy•of•systemic•sirolimus•administration•to•prevent•neointimal\"}, # 75 token long 1-shot sentence\n",
    "            {\"role\": \"assistant\", \"content\": \"0•0•0•0•I-Control•0•0•0•0•0•O-Physical•0•0•0•0•P-Condition•P-Condition•0•0•0•I-Drug•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•0•O-Physical\"}, # 75 token long 1-shot annotation\n",
    "            {\"role\": \"user\", \"content\": sentence}  # Sentence to annotate\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    gpt_annotations = completion.choices[0].message.content.split('•')\n",
    "\n",
    "    if len(gpt_annotations) > chunk_size:\n",
    "        gpt_annotations = gpt_annotations[:chunk_size]\n",
    "    elif len(gpt_annotations) < chunk_size:\n",
    "        gpt_annotations += ['0'] * (chunk_size - len(gpt_annotations))\n",
    "    \n",
    "    return '•'.join(gpt_annotations)\n",
    "\n",
    "\n",
    "def process_annotations(gpt_df):\n",
    "    results = pd.DataFrame(columns=['sentence', 'gpt_annotation', 'gold_annotation'])\n",
    "\n",
    "    # Iteration over each element of the Dataframe (per PMID)\n",
    "    for index, row in tqdm(GPT_df.iloc[:99].iterrows(), total=3, desc=\"Processing annotations\"):\n",
    "        sentences = row['sentence'].split('•')\n",
    "        annotations = row['annotation'].split('•')\n",
    "        \n",
    "        temp_sentences = []\n",
    "        temp_gpt_annotations = []\n",
    "        temp_gold_annotations = []\n",
    "\n",
    "        # Chuncking is taking part underneath this iteration\n",
    "        for i in range(0, len(sentences), chunk_size):\n",
    "            sentence_chunk = '•'.join(sentences[i:i+chunk_size])\n",
    "            annotation_chunk = '•'.join(annotations[i:i+chunk_size])\n",
    "            \n",
    "            # Utilizing the API call to classify\n",
    "            gpt_annotation = get_annotations(sentence_chunk)\n",
    "            \n",
    "            # Storing results\n",
    "            temp_sentences.append(sentence_chunk)\n",
    "            temp_gpt_annotations.append(gpt_annotation)\n",
    "            temp_gold_annotations.append(annotation_chunk)\n",
    "\n",
    "        # Merging results with gold data, to evaluate\n",
    "        results = pd.concat([results, pd.DataFrame({\n",
    "            'sentence': ['•'.join(temp_sentences).split(\"•\")],\n",
    "            'gpt_annotation': ['•'.join(temp_gpt_annotations).split(\"•\")],\n",
    "            'gold_annotation': ['•'.join(temp_gold_annotations).split(\"•\")]\n",
    "        })], ignore_index=True)\n",
    "\n",
    "    return results\n",
    "\n",
    "results = process_annotations(GPT_df)\n",
    "results.to_csv('fulllenght_annotations_results.csv', index=True) # Saving the results for later evaluation\n",
    "print(results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Token-level Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "                0       0.83      0.90      0.87     10869\n",
      "           I-Drug       0.02      0.03      0.03        30\n",
      "    I-Educational       0.00      0.00      0.00        86\n",
      "          I-Other       0.02      0.01      0.01       170\n",
      "       I-Physical       0.00      0.00      0.00        98\n",
      "  I-Psychological       0.05      0.01      0.01       119\n",
      "       I-Surgical       0.04      0.03      0.03       120\n",
      "O-Adverse effects       0.08      0.07      0.08        83\n",
      "         O-Mental       0.09      0.11      0.10       149\n",
      "       O-Morality       0.00      0.00      0.00         5\n",
      "          O-Other       0.04      0.02      0.03       256\n",
      "       O-Physical       0.13      0.04      0.06       603\n",
      "            P-Age       0.04      0.03      0.03        40\n",
      "      P-Condition       0.08      0.07      0.08       417\n",
      "    P-Sample size       0.08      0.06      0.07        99\n",
      "            P-Sex       0.00      0.00      0.00        16\n",
      "\n",
      "        micro avg       0.76      0.75      0.76     13160\n",
      "        macro avg       0.09      0.09      0.09     13160\n",
      "     weighted avg       0.70      0.75      0.73     13160\n",
      "      samples avg       0.75      0.75      0.75     13160\n",
      "\n",
      "Accuracy: 0.7545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:895: UserWarning: unknown class(es) ['', '2020', 'I-Control', 'O-Mortality', 'P-Other', 'P-Year'] will be ignored\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "gold_labels_flat = [label for sublist in results['gold_annotation'] for label in sublist]\n",
    "predicted_labels_flat = [label for sublist in results['gpt_annotation'] for label in sublist]\n",
    "\n",
    "min_length = min(len(gold_labels_flat), len(predicted_labels_flat))\n",
    "gold_labels_flat = gold_labels_flat[:min_length]\n",
    "predicted_labels_flat = predicted_labels_flat[:min_length]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "gold_labels_binarized = mlb.fit_transform([[label] for label in gold_labels_flat])\n",
    "predicted_labels_binarized = mlb.transform([[label] for label in predicted_labels_flat])\n",
    "\n",
    "print(classification_report(gold_labels_binarized, predicted_labels_binarized, target_names=mlb.classes_))\n",
    "\n",
    "token_accuracy = accuracy_score(gold_labels_binarized, predicted_labels_binarized)\n",
    "print(f\"Accuracy: {token_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Manual Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "def print_formatted_data_html(df, lstm_predictions, output_file='fulllenght_output.html'):\n",
    "    html_output = \"<html><head><title>Annotated Sentences</title></head><body>\"\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        sentence_html = f\"<h3>Sentence {idx+1}</h3><div style='display: flex; flex-wrap: wrap; gap: 20px; align-items: flex-start;'>\"\n",
    "        sentence_words = row['sentence']\n",
    "        gpt_annotations = row['gpt_annotation']\n",
    "        gold_annotations = row['gold_annotation']\n",
    "        lstm_annotations = [label for token, label in lstm_predictions[idx]]\n",
    "        \n",
    "        for word_idx, word in enumerate(sentence_words):\n",
    "            gpt_annotation = gpt_annotations[word_idx].strip() if word_idx < len(gpt_annotations) else '!'\n",
    "            lstm_annotation = lstm_annotations[word_idx].strip() if word_idx < len(lstm_annotations) else '!'\n",
    "            gold_annotation = gold_annotations[word_idx].strip() if word_idx < len(gold_annotations) else '!'\n",
    "            \n",
    "            gpt_color = \"red\" if gpt_annotation != gold_annotation else \"black\"\n",
    "            lstm_color = \"orange\" if lstm_annotation != gold_annotation else \"black\"\n",
    "            \n",
    "            word_html = f\"<div style='text-align: center;'>\"\n",
    "            word_html += f\"<div style='color: black; font-weight: bold;'>{word.strip()}</div>\"\n",
    "            word_html += f\"<div style='color: {gpt_color};'>GPT: {gpt_annotation}</div>\"\n",
    "            word_html += f\"<div style='color: {lstm_color};'>LSTM: {lstm_annotation}</div>\"\n",
    "            word_html += f\"<div style='color: black;'>Gold: {gold_annotation}</div>\"\n",
    "            word_html += \"</div>\"\n",
    "            sentence_html += word_html\n",
    "        \n",
    "        sentence_html += \"</div>\"\n",
    "        html_output += sentence_html\n",
    "    \n",
    "    html_output += \"</body></html>\"\n",
    "    \n",
    "    # Saving it to a file\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(html_output)\n",
    "    \n",
    "    # Displaing it within this jupyter notebook\n",
    "    display(HTML(html_output))\n",
    "\n",
    "print_formatted_data_html(results, decoded_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
