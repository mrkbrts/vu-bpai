{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Bachelor Project Artificial Intelligence**\n",
    "\n",
    "M Bartos (2724195)\n",
    "\n",
    "Vrije Universiteit Amsterdam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Preprocessing the Train Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def read_data(directory):\n",
    "    data = []\n",
    "    for subdir, _, files in os.walk(os.path.join(directory, 'documents')):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.text'):\n",
    "                pmid = filename.split('.')[0]\n",
    "                with open(os.path.join(subdir, filename), 'r') as file:\n",
    "                    text = file.read().strip()\n",
    "                tokens_file = os.path.join(subdir, f\"{pmid}.tokens\")\n",
    "                with open(tokens_file, 'r') as file:\n",
    "                    tokens = file.read().strip().split()\n",
    "                data.append({'pmid': pmid, 'text': text, 'tokens': tokens})\n",
    "    return data\n",
    "\n",
    "def parse_pico(tokens, labels_p_file, labels_i_file, labels_o_file):\n",
    "    pico_annotations = []\n",
    "    with open(labels_p_file, 'r') as p_file, open(labels_i_file, 'r') as i_file, open(labels_o_file, 'r') as o_file:\n",
    "        labels_p = p_file.read().strip().split(',')\n",
    "        labels_i = i_file.read().strip().split(',')\n",
    "        labels_o = o_file.read().strip().split(',')\n",
    "\n",
    "    for token, p, i, o in zip(tokens, labels_p, labels_i, labels_o):\n",
    "        pico_annotations.append({\n",
    "            'token': token,\n",
    "            'P': int(p),\n",
    "            'I': int(i),\n",
    "            'O': int(o)\n",
    "        })\n",
    "    return pico_annotations\n",
    "\n",
    "data_directory = '/Users/markbartos/Library/Mobile Documents/com~apple~CloudDocs/DRIVE/EDUCATION/VU_AI/YEAR3 PERIOD 5/BPAI/Code/EBM-NLP-master/ebm_nlp_1_00'\n",
    "\n",
    "data = read_data(data_directory)\n",
    "\n",
    "sentence_data = []\n",
    "for entry in data:\n",
    "    pmid = entry['pmid']\n",
    "    tokens = entry['tokens']\n",
    "    \n",
    "    labels_p_file = os.path.join(data_directory, 'annotations', 'aggregated', 'hierarchical_labels', 'participants', 'train', f\"{pmid}_AGGREGATED.ann\")\n",
    "    labels_i_file = os.path.join(data_directory, 'annotations', 'aggregated', 'hierarchical_labels', 'interventions', 'train', f\"{pmid}_AGGREGATED.ann\")\n",
    "    labels_o_file = os.path.join(data_directory, 'annotations', 'aggregated', 'hierarchical_labels', 'outcomes', 'train', f\"{pmid}_AGGREGATED.ann\")\n",
    "\n",
    "    if os.path.exists(labels_p_file) and os.path.exists(labels_i_file) and os.path.exists(labels_o_file):\n",
    "        annotations = parse_pico(tokens, labels_p_file, labels_i_file, labels_o_file)\n",
    "        sentence_data.append({'PMID': pmid, 'Annotations': annotations})\n",
    "\n",
    "\n",
    "df = pd.DataFrame(sentence_data)\n",
    "#print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Preprocessing the Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def read_data(directory):\n",
    "    data = []\n",
    "    for subdir, _, files in os.walk(os.path.join(directory, 'documents')):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.text'):\n",
    "                pmid = filename.split('.')[0]\n",
    "                with open(os.path.join(subdir, filename), 'r') as file:\n",
    "                    text = file.read().strip()\n",
    "                tokens_file = os.path.join(subdir, f\"{pmid}.tokens\")\n",
    "                with open(tokens_file, 'r') as file:\n",
    "                    tokens = file.read().strip().split()\n",
    "                data.append({'pmid': pmid, 'text': text, 'tokens': tokens})\n",
    "    return data\n",
    "\n",
    "def parse_pico(tokens, labels_p_file, labels_i_file, labels_o_file):\n",
    "    pico_annotations = []\n",
    "    with open(labels_p_file, 'r') as p_file, open(labels_i_file, 'r') as i_file, open(labels_o_file, 'r') as o_file:\n",
    "        labels_p = p_file.read().strip().split(',')\n",
    "        labels_i = i_file.read().strip().split(',')\n",
    "        labels_o = o_file.read().strip().split(',')\n",
    "\n",
    "    for token, p, i, o in zip(tokens, labels_p, labels_i, labels_o):\n",
    "        pico_annotations.append({\n",
    "            'token': token,\n",
    "            'P': int(p),\n",
    "            'I': int(i),\n",
    "            'O': int(o)\n",
    "        })\n",
    "    return pico_annotations\n",
    "\n",
    "data_directory = '/Users/markbartos/Library/Mobile Documents/com~apple~CloudDocs/DRIVE/EDUCATION/VU_AI/YEAR3 PERIOD 5/BPAI/Code/EBM-NLP-master/ebm_nlp_1_00'\n",
    "\n",
    "data = read_data(data_directory)\n",
    "#print(data[0])\n",
    "\n",
    "sentence_data = []\n",
    "for entry in data:\n",
    "    pmid = entry['pmid']\n",
    "    tokens = entry['tokens']\n",
    "    \n",
    "    labels_p_file = os.path.join(data_directory, 'annotations', 'aggregated', 'hierarchical_labels', 'participants', 'test', 'gold', f\"{pmid}_AGGREGATED.ann\")\n",
    "    labels_i_file = os.path.join(data_directory, 'annotations', 'aggregated', 'hierarchical_labels', 'interventions', 'test', 'gold', f\"{pmid}_AGGREGATED.ann\")\n",
    "    labels_o_file = os.path.join(data_directory, 'annotations', 'aggregated', 'hierarchical_labels', 'outcomes', 'test', 'gold', f\"{pmid}_AGGREGATED.ann\")\n",
    "\n",
    "    if os.path.exists(labels_p_file) and os.path.exists(labels_i_file) and os.path.exists(labels_o_file):\n",
    "        annotations = parse_pico(tokens, labels_p_file, labels_i_file, labels_o_file)\n",
    "        sentence_data.append({'PMID': pmid, 'Annotations': annotations})\n",
    "\n",
    "\n",
    "test_df = pd.DataFrame(sentence_data)\n",
    "#print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Inspecting the Database**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the desiered lines. Keep commented out for clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Per PMID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)  \n",
    "#print(df.iloc[0])\n",
    "#print(test_df.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Annotations per token**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = df.at[0, 'Annotations']\n",
    "annotations = test_df.at[0, 'Annotations']\n",
    "\n",
    "for annotation in annotations:\n",
    "    #print(f\"Token: {annotation['token']} - P: {annotation['P']}, I: {annotation['I']}, O: {annotation['O']}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Token Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_list = df.at[0, 'Annotations']\n",
    "annotations_df = pd.DataFrame(annotations_list)\n",
    "sorted_annotations_df = annotations_df.sort_values(by=['P', 'I', 'O'])\n",
    "#print(sorted_annotations_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training the biLSTM model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating a df suitable for LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pico_map = {\n",
    "    'P': ['No label', 'Age', 'Sex', 'Sample size', 'Condition'],\n",
    "    'I': ['No label', 'Surgical', 'Physical', 'Drug', 'Educational', 'Psychological', 'Other', 'Control'],\n",
    "    'O': ['No label', 'Physical', 'Pain', 'Mortality', 'Adverse effects', 'Mental', 'Other']\n",
    "}\n",
    "\n",
    "def map_annotations(annotation):\n",
    "    mapped_p = pico_map['P'][annotation['P']] if annotation['P'] < len(pico_map['P']) else 'No label'\n",
    "    mapped_i = pico_map['I'][annotation['I']] if annotation['I'] < len(pico_map['I']) else 'No label'\n",
    "    mapped_o = pico_map['O'][annotation['O']] if annotation['O'] < len(pico_map['O']) else 'No label'\n",
    "\n",
    "    result = []\n",
    "    if mapped_p != 'No label':\n",
    "        result.append(f\"P-{mapped_p}\")\n",
    "    elif mapped_i != 'No label':\n",
    "        result.append(f\"I-{mapped_i}\")\n",
    "    elif mapped_o != 'No label':\n",
    "        result.append(f\"O-{mapped_o}\")\n",
    "\n",
    "    return ','.join(result) if result else '0'\n",
    "\n",
    "def create_lstm_df(df):\n",
    "    lstm_data = []\n",
    "    for index, row in df.iterrows():\n",
    "        tokens = [token['token'] for token in row['Annotations']]\n",
    "        annotations = [map_annotations(token) for token in row['Annotations']]\n",
    "        lstm_data.append({\"PMID\": pmid, \"Tokens\": tokens, \"Annotations\": annotations})\n",
    "    \n",
    "    return pd.DataFrame(lstm_data)\n",
    "\n",
    "\n",
    "LSTM_df = create_lstm_df(df)\n",
    "LSTM_test_df = create_lstm_df(test_df)\n",
    "#print(LSTM_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training the BiLSTM model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 500)]             0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 500, 50)           1887500   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 500, 50)           0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 500, 200)          120800    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDi  (None, 500, 50)           10050     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDi  (None, 500, 19)           969       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2019319 (7.70 MB)\n",
      "Trainable params: 2019319 (7.70 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "def prepare_data(df):\n",
    "    sentences = df['Tokens'].tolist()\n",
    "    labels = df['Annotations'].tolist()\n",
    "    return sentences, labels\n",
    "\n",
    "train_sentences, train_labels = prepare_data(LSTM_df)\n",
    "test_sentences, test_labels = prepare_data(LSTM_test_df)\n",
    "\n",
    "def build_model(max_len, n_words, n_tags):\n",
    "    input = Input(shape=(max_len,))\n",
    "    model = Embedding(input_dim=n_words, output_dim=50, input_length=max_len)(input)\n",
    "    model = Dropout(0.1)(model)\n",
    "    model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "    model = TimeDistributed(Dense(50, activation=\"relu\"))(model)\n",
    "    out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(model)\n",
    "    model = Model(input, out)\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "max_len = 500 # This is a cut-off value, to speed up training but it does increase loss\n",
    "n_words = len(set([token for sentence in train_sentences for token in sentence])) + 1\n",
    "n_tags = len(set([annotation for labels in train_labels for annotation in labels])) + 1\n",
    "\n",
    "model = build_model(max_len, n_words, n_tags)\n",
    "model.summary()\n",
    "\n",
    "def encode_data(sentences, labels, max_len, n_words, n_tags):\n",
    "    word2idx = {w: i for i, w in enumerate(set([token for sentence in sentences for token in sentence]), 1)}\n",
    "    label2idx = {l: i for i, l in enumerate(set([label for label_list in labels for label in label_list]), 1)}\n",
    "    \n",
    "    X = [[word2idx[token] for token in sentence] for sentence in sentences]\n",
    "    X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=n_words-1)\n",
    "    \n",
    "    y = [[label2idx[label] for label in label_list] for label_list in labels]\n",
    "    y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=n_tags-1)\n",
    "    \n",
    "    return X, np.array(y), word2idx, label2idx\n",
    "\n",
    "# Encode training data\n",
    "X_train, y_train, word2idx, label2idx = encode_data(train_sentences, train_labels, max_len, n_words, n_tags)\n",
    "X_test, y_test, _, _ = encode_data(test_sentences, test_labels, max_len, n_words, n_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "79/79 [==============================] - 36s 444ms/step - loss: 0.9258 - accuracy: 0.8244 - val_loss: 0.5011 - val_accuracy: 0.8970\n",
      "Epoch 2/10\n",
      "79/79 [==============================] - 37s 474ms/step - loss: 0.4927 - accuracy: 0.8958 - val_loss: 0.4714 - val_accuracy: 0.8977\n",
      "Epoch 3/10\n",
      "79/79 [==============================] - 38s 480ms/step - loss: 0.4385 - accuracy: 0.8966 - val_loss: 0.4236 - val_accuracy: 0.8989\n",
      "Epoch 4/10\n",
      "79/79 [==============================] - 37s 468ms/step - loss: 0.3872 - accuracy: 0.9009 - val_loss: 0.4016 - val_accuracy: 0.8996\n",
      "Epoch 5/10\n",
      "79/79 [==============================] - 38s 476ms/step - loss: 0.3512 - accuracy: 0.9077 - val_loss: 0.3977 - val_accuracy: 0.9011\n",
      "Epoch 6/10\n",
      "79/79 [==============================] - 38s 477ms/step - loss: 0.3243 - accuracy: 0.9129 - val_loss: 0.3989 - val_accuracy: 0.9010\n",
      "Epoch 7/10\n",
      "79/79 [==============================] - 39s 494ms/step - loss: 0.3002 - accuracy: 0.9175 - val_loss: 0.3975 - val_accuracy: 0.9010\n",
      "Epoch 8/10\n",
      "79/79 [==============================] - 39s 491ms/step - loss: 0.2767 - accuracy: 0.9210 - val_loss: 0.3891 - val_accuracy: 0.9016\n",
      "Epoch 9/10\n",
      "79/79 [==============================] - 39s 489ms/step - loss: 0.2512 - accuracy: 0.9258 - val_loss: 0.3889 - val_accuracy: 0.9006\n",
      "Epoch 10/10\n",
      "79/79 [==============================] - 37s 473ms/step - loss: 0.2258 - accuracy: 0.9316 - val_loss: 0.3925 - val_accuracy: 0.8953\n",
      "4/4 [==============================] - 0s 103ms/step - loss: 1.0304 - accuracy: 0.8154\n",
      "Loss: 1.0304228067398071, Accuracy: 0.815353512763977\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=32, epochs=10, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Predicting for the test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 103ms/step\n",
      "Label index to name mapping: {1: 'P-Sample size', 2: '0', 3: 'I-Drug', 4: 'P-Age', 5: 'I-Educational', 6: 'I-Physical', 7: 'P-Sex', 8: 'I-Control', 9: 'O-Mental', 10: 'O-Physical', 11: 'O-Pain', 12: 'O-Mortality', 13: 'I-Other', 14: 'O-Other', 15: 'I-Surgical', 16: 'P-Condition', 17: 'O-Adverse effects', 18: 'I-Psychological'}\n"
     ]
    }
   ],
   "source": [
    "# Predicting labels\n",
    "predictions = model.predict(X_test, verbose=1)\n",
    "\n",
    "# Converting predictions from encoded back to the original labels\n",
    "predicted_labels = np.argmax(predictions, axis=-1)\n",
    "idx2label = {i: label for label, i in label2idx.items()}\n",
    "print(\"Label index to name mapping:\", idx2label)\n",
    "\n",
    "def decode_predictions(predictions, sentences, idx2label):\n",
    "    decoded_predictions = []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        decoded_sentence = []\n",
    "        for j, token in enumerate(sentence):\n",
    "            if j < len(predictions[i]):\n",
    "                label_idx = predictions[i][j]\n",
    "                label = idx2label.get(label_idx, 'O')  # O if label not found\n",
    "                decoded_sentence.append((token, label))\n",
    "            else:\n",
    "                decoded_sentence.append((token, 'O'))  # O if label not found\n",
    "        decoded_predictions.append(decoded_sentence)\n",
    "    return decoded_predictions\n",
    "\n",
    "# Raise error if label not in range\n",
    "for label in predicted_labels.flatten():\n",
    "    if label not in idx2label:\n",
    "        print(f\"Warning: Predicted label index {label} not found in idx2label mapping.\")\n",
    "\n",
    "decoded_predictions = decode_predictions(predicted_labels, test_sentences, idx2label)\n",
    "\n",
    "def create_prediction_df(df, decoded_predictions):\n",
    "    prediction_data = []\n",
    "    for index, row in df.iterrows():\n",
    "        pmid = row[\"PMID\"]\n",
    "        tokens = row[\"Tokens\"]\n",
    "        annotations = [label for _, label in decoded_predictions[index]]\n",
    "        prediction_data.append({\"PMID\": pmid, \"Tokens\": tokens, \"Predictions\": annotations})\n",
    "    \n",
    "    return pd.DataFrame(prediction_data)\n",
    "\n",
    "# Implementing it back into a df\n",
    "prediction_df = create_prediction_df(LSTM_test_df, decoded_predictions)\n",
    "#print(prediction_df.head())\n",
    "\n",
    "# Ability to manually examine predictions\n",
    "for i in range(0): # Increase range to get started\n",
    "    print(f\"Sentence {i+1}:\")\n",
    "    for token, label in decoded_predictions[i]:\n",
    "        print(f\"{token}: {label}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluating for LSTM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Absract-level Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "                0       1.00      1.00      1.00        99\n",
      "        I-Control       0.00      0.00      0.00        36\n",
      "           I-Drug       0.66      0.63      0.65        65\n",
      "    I-Educational       0.20      0.08      0.12        12\n",
      "          I-Other       0.00      0.00      0.00        15\n",
      "       I-Physical       0.14      0.05      0.08        19\n",
      "  I-Psychological       0.04      1.00      0.08         4\n",
      "       I-Surgical       0.00      0.00      0.00        16\n",
      "O-Adverse effects       0.00      0.00      0.00        29\n",
      "         O-Mental       0.11      0.06      0.07        18\n",
      "      O-Mortality       0.00      0.00      0.00        11\n",
      "          O-Other       0.33      0.02      0.04        53\n",
      "           O-Pain       0.00      0.00      0.00         5\n",
      "       O-Physical       0.82      0.18      0.29        79\n",
      "            P-Age       0.00      0.00      0.00        30\n",
      "      P-Condition       0.00      0.00      0.00        91\n",
      "    P-Sample size       0.00      0.00      0.00        84\n",
      "            P-Sex       0.00      0.00      0.00        20\n",
      "\n",
      "        micro avg       0.54      0.24      0.33       686\n",
      "        macro avg       0.18      0.17      0.13       686\n",
      "     weighted avg       0.34      0.24      0.25       686\n",
      "      samples avg       0.54      0.24      0.33       686\n",
      "\n",
      "Accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:895: UserWarning: unknown class(es) ['O'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "def abstract_level_evaluation(prediction_df, LSTM_test_df):\n",
    "    mlb = MultiLabelBinarizer()\n",
    "\n",
    "    # Prepare gold labels\n",
    "    gold_labels = [set(row['Annotations']) for _, row in LSTM_test_df.iterrows()]\n",
    "    predicted_labels = [set(row['Predictions']) for _, row in prediction_df.iterrows()]\n",
    "\n",
    "    # Binarize the labels\n",
    "    gold_labels_binarized = mlb.fit_transform(gold_labels)\n",
    "    predicted_labels_binarized = mlb.transform(predicted_labels)\n",
    "\n",
    "    # Generate the classification report\n",
    "    report = classification_report(gold_labels_binarized, predicted_labels_binarized, target_names=mlb.classes_, zero_division=0)\n",
    "    print(report)\n",
    "\n",
    "    accuracy = accuracy_score(gold_labels_binarized, predicted_labels_binarized)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Assuming prediction_df and LSTM_test_df are already created and loaded as per your previous steps\n",
    "abstract_level_evaluation(prediction_df, LSTM_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Token-level Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "      O-Mortality       0.00      0.00      0.00        90\n",
      "        I-Control       0.00      0.00      0.00      1748\n",
      "      P-Condition       0.04      0.01      0.02       676\n",
      "                0       0.80      0.75      0.77     22846\n",
      "       O-Physical       0.00      0.00      0.00        25\n",
      "            P-Age       0.00      0.00      0.00       100\n",
      "       I-Surgical       0.00      0.00      0.00       201\n",
      "    I-Educational       0.00      0.00      0.00       295\n",
      "         O-Mental       0.02      0.01      0.01       341\n",
      "           O-Pain       0.00      0.00      0.00       120\n",
      "            P-Sex       0.00      0.00      0.00        49\n",
      "          O-Other       0.00      0.00      0.00       555\n",
      "       I-Physical       0.00      0.02      0.01       253\n",
      "O-Adverse effects       0.00      0.00      0.00       144\n",
      "    P-Sample size       0.00      0.00      0.00       161\n",
      "          I-Other       0.00      0.00      0.00       120\n",
      "           I-Drug       0.03      0.11      0.04       876\n",
      "  I-Psychological       0.93      1.00      0.96     20900\n",
      "\n",
      "         accuracy                           0.77     49500\n",
      "        macro avg       0.10      0.11      0.10     49500\n",
      "     weighted avg       0.76      0.77      0.76     49500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "true_labels = y_test.flatten()\n",
    "pred_labels = predicted_labels.flatten()\n",
    "\n",
    "# Convert the labels back to their original names\n",
    "true_label_names = [idx2label[idx] for idx in true_labels]\n",
    "pred_label_names = [idx2label[idx] for idx in pred_labels]\n",
    "\n",
    "report = classification_report(true_label_names, pred_label_names, labels=list(idx2label.values()), zero_division=0, digits=2)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setting up the GPT-4 model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating a suitable df for GPT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pico_map = {\n",
    "    'P': ['No label', 'Age', 'Sex', 'Sample size', 'Condition'],\n",
    "    'I': ['No label', 'Surgical', 'Physical', 'Drug', 'Educational', 'Psychological', 'Other', 'Control'],\n",
    "    'O': ['No label', 'Physical', 'Pain', 'Mortality', 'Adverse effects', 'Mental', 'Other']\n",
    "}\n",
    "\n",
    "def map_annotations(annotation):\n",
    "    mapped_p = pico_map['P'][annotation['P']] if annotation['P'] < len(pico_map['P']) else 'No label'\n",
    "    mapped_i = pico_map['I'][annotation['I']] if annotation['I'] < len(pico_map['I']) else 'No label'\n",
    "    mapped_o = pico_map['O'][annotation['O']] if annotation['O'] < len(pico_map['O']) else 'No label'\n",
    "\n",
    "    result = []\n",
    "    if mapped_p != 'No label':\n",
    "        result.append(f\"P-{mapped_p}\")\n",
    "    elif mapped_i != 'No label':\n",
    "        result.append(f\"I-{mapped_i}\")\n",
    "    elif mapped_o != 'No label':\n",
    "        result.append(f\"O-{mapped_o}\")\n",
    "\n",
    "    return ','.join(result) if result else '0'\n",
    "\n",
    "def create_gpt_df(df):\n",
    "    gpt_data = []\n",
    "    for index, row in df.iterrows():\n",
    "        sentence = '•'.join([token['token'] for token in row['Annotations']])\n",
    "        annotation = '•'.join([map_annotations(token) for token in row['Annotations']])\n",
    "        gpt_data.append({\"sentence\": sentence, \"annotation\": annotation})\n",
    "    \n",
    "    return pd.DataFrame(gpt_data)\n",
    "\n",
    "GPT_df = create_gpt_df(test_df)\n",
    "#print(GPT_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manual Examination of Database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)  \n",
    "#print(GPT_df.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Predicting with GPT-4o**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing annotations: 100%|██████████| 99/99 [22:52<00:00, 13.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      sentence   \n",
      "0                                                                                                                     [The, acute, effects, of, fluid, intake, on, urine, specific, gravity, and, fluid, retention, in, a, mildly, dehydrated, state, ., Many, athletes, arrive, at, training, sessions, and, competitions, in, a, mildly, hypohydrated, (, HYPO, ), state, and, are, instructed, to, drink, fluids, before, exercise, to, reach, a, euhydrated, (, HYD, ), state, ., Ten, recreational, athletes, (, 6, women, ,, 4, men, ;, 71.9, ?, 4.6, kg, ,, 25.2, ?, 0.9, years, ), participated, in, the, studies, to, examine, (, a, ), the, day-to-day, variability, of, morning, urine, specific, gravity, (, USG, ), ,, (, b, ), the, effects, of, consuming, ...]  \\\n",
      "1                                                                                 [Pedantic, speaking, style, differentiates, Asperger, syndrome, from, high-functioning, autism, ., Asperger, syndrome, (, AS, ), is, a, pervasive, developmental, disorder, recently, introduced, as, a, new, diagnostic, category, in, the, ICD-10, and, the, DSM-IV, ., Along, with, motor, clumsiness, ,, pedantic, speech, has, been, proposed, as, a, clinical, feature, of, AS, ., However, ,, few, attempts, have, been, made, to, define, and, measure, this, symptom, ., We, studied, 17, patients, with, AS, (, ICD-10, ;, 14, male, ,, 3, female, ;, mean, age, 16.4, years, ,, mean, full-scale, IQ, 97, ), and, compared, them, with, a, control, group, of, 13, patients, ...]   \n",
      "2  [Timing, for, delivering, individualized, patient, education, intervention, to, Coronary, Artery, Bypass, Graft, patients, :, An, RCT, ., BACKGROUND, The, primary, focus, of, this, study, is, on, the, timing, of, the, delivery, of, education, to, patients, who, had, CABG, surgery, ., AIM, To, determine, the, efficacy, of, an, individualized, telephone, patient, education, intervention, ,, delivered, at, two, different, points, in, time, (, 1-2, days, pre-discharge, versus, 1-2, days, post-discharge, ), in, enhancing, the, CABG, patient, 's, knowledge, of, self-care, behaviours, ,, performance, of, self-care, behaviours, ,, and, symptom, frequency, ., METHOD, A, randomized, clinical, trial, that, included, a, convenience, sample, of, ...]   \n",
      "3                 [Sunbathing, and, sunbed, use, related, to, self-image, in, a, randomized, sample, of, Swedish, adolescents, ., In, 1996, a, randomized, sample, of, 4,020, Swedish, adolescents, from, three, birth, cohorts, were, sent, a, questionnaire, consisting, of, 50, items, concerning, habitual, sun-related, behaviours, and, attitudes, ,, knowledge, about, melanoma, ,, risk, perception, and, self-image, ., A, total, of, 2,615, questionnaires, were, returned, ., Girls, sunbathed, and, used, sunbeds, more, than, boys, at, all, ages, ., Sunbathing, and, sunbed, use, increased, with, age, ., Boys, who, were, most, satisfied, and, girls, least, satisfied, with, themselves, sunbathed, most, ., Those, who, were, least, satisfied, with, ...]   \n",
      "4                             [Study, of, the, vaginal, tolerance, to, Acidform, ,, an, acid-buffering, ,, bioadhesive, gel, ., Vaginal, tolerance, tests, were, performed, with, a, new, potential, microbicidal, and, spermicidal, product, ,, an, acid-buffering, vaginal, gel, (, Acidform, ), without, or, with, nonoxynol-9, (, N-9, ), ., The, potential, advantages, over, other, vaginal, products, include, keeping, a, low, pH, ,, decrease, of, the, irritating, effect, of, N-9, on, the, cervix, or, vaginal, mucosa, associated, with, greater, retention, of, the, product, after, application, ,, and, decreasing, messiness, as, compared, to, other, vaginal, products, ., Three, groups, of, six, women, were, admitted, and, randomly, assigned, to, ...]   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 gpt_annotation   \n",
      "0  [0, 0, 0, 0, O-Physical, 0, 0, 0, 0, 0, P-Condition, 0, 0, 0, 0, 0, P-Condition, P-Condition, 0, 0, 0, 0, 0, P-Sample size, 0, P-Sex, 0, P-Sex, 0, P-Condition, 0, 0, 0, P-Condition, 0, 0, 0, P-Condition, 0, P-Sample size, P-Condition, 0, P-Sex, 0, P-Sex, 0, P-Sample size, 0, 0, P-Age, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, O-Physical, 0, 0, 0, 0, 0, O-Physical, 0, 0, 0, 0, 0, I-Other, 0, 0, P-Sample size, 0, 0, 0, ...]  \\\n",
      "1  [0, 0, 0, 0, P-Condition, 0, 0, P-Condition, 0, 0, P-Condition, 0, 0, 0, 0, 0, P-Condition, 0, 0, P-Condition, 0, 0, 0, 0, 0, 0, 0, 0, P-Condition, 0, 0, 0, 0, 0, P-Other, 0, P-Condition, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, P-Sample size, 0, 0, 0, 0, 0, P-Condition, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, P-Condition, , 0, 0, 0, 0, P-Sex, 0, P-Sex, 0, 0, P-Age, P-Age, 0, 0, P-Condition, P-Condition, 0, 0, 0, 0, P-Sample size, 0, 0, 0, P-Sex, P-Sex, 0, P-Age, P-Age, 0, ...]   \n",
      "2                                [0, 0, 0, 0, I-Educational, 0, 0, P-Condition, 0, 0, 0, 0, 0, 0, 0, I-Control, 0, 0, 0, 0, 0, 0, P-Condition, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, I-Educational, 0, 0, P-Condition, I-Surgical, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, I-Educational, 0, 0, 0, 0, 0, 0, 0, 0, I-Educational, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, O-Other, 0, 0, 0, 0, 0, O-Other, 0, 0, 0, P-Sample size, 0, 0, 0, 0, P-Condition, 0, 0, 0, 0, I-Educational, 0, 0, 0, 0, ...]   \n",
      "3                                                    [0, 0, 0, 0, 0, 0, 0, 0, P-Sample size, 0, P-Condition, P-Sample size, 0, 0, 0, P-Age, 0, 0, 0, 0, 0, 0, 0, P-Sample size, P-Condition, 0, 0, 0, 0, 0, 0, 0, 0, 0, P-Mental, 0, 0, 0, 0, I-Educational, 0, 0, 0, 0, 0, 0, P-Sample size, 0, 0, 0, P-Sex, 0, 0, 0, 0, 0, P-Sex, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, P-Age, 0, 0, 0, 0, 0, 0, 0, 0, O-Other, 0, P-Age, 0, P-Sex, 0, 0, 0, P-Sex, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]   \n",
      "4                                                             [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, P-Condition, 0, 0, 0, 0, 0, 0, P-Condition, 0, I-Drug, 0, 0, 0, 0, 0, 0, I-Drug, 0, 0, 0, I-Drug, 0, 0, 0, 0, 0, 0, I-Drug, 0, 0, 0, 0, 0, 0, 0, O-Adverse effects, 0, O-Adverse effects, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, O-Adverse effects, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, , 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, P-Sex, 0, P-Sample size, 0, 0, 0, 0, 0, P-Sex, 0, 0, 0, 0, 0, I-Drug, ...]   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                gold_annotation  \n",
      "0                                                                                                                                                     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, P-Condition, P-Condition, P-Condition, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, P-Condition, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]  \n",
      "1                                                               [I-Psychological, I-Psychological, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, I-Psychological, I-Psychological, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, P-Sample size, 0, 0, P-Condition, 0, 0, 0, P-Sample size, P-Sex, 0, P-Sample size, P-Sex, 0, 0, 0, P-Age, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, P-Sample size, 0, ...]  \n",
      "2  [0, 0, 0, I-Educational, I-Educational, I-Educational, I-Educational, 0, P-Condition, P-Condition, P-Condition, P-Condition, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, P-Condition, P-Condition, I-Surgical, 0, 0, 0, 0, 0, 0, 0, I-Educational, I-Educational, I-Educational, I-Educational, I-Educational, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, P-Condition, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]  \n",
      "3                                                                                                                         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, P-Age, 0, 0, 0, 0, 0, 0, 0, P-Sample size, 0, P-Age, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, P-Sample size, 0, 0, 0, 0, P-Sex, 0, 0, 0, 0, 0, 0, P-Sex, 0, 0, 0, 0, O-Mental, 0, O-Mental, O-Mental, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, O-Mental, 0, 0, 0, 0, 0, 0, 0, 0, ...]  \n",
      "4                                                          [0, 0, 0, 0, 0, 0, I-Drug, I-Drug, I-Drug, I-Drug, I-Drug, I-Drug, I-Drug, I-Drug, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, I-Drug, I-Drug, I-Drug, I-Drug, I-Drug, I-Drug, I-Drug, I-Drug, I-Drug, I-Drug, I-Drug, I-Drug, I-Drug, I-Drug, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, I-Drug, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, P-Sample size, P-Sex, 0, 0, 0, 0, 0, 0, ...]  \n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "openai.api_key = \"sk-proj-oiuxDclszqeK2nXu4n9CT3BlbkFJttrqrKBM9FCiXJjJPHvp\"\n",
    "chunk_size = 75\n",
    "\n",
    "\n",
    "def get_annotations(sentence):\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a PICO annotator tasked with labeling tokens. For every chunk of 75 tokens separated by bullets (•), return 75 PICO annotations. Use one of the following options: 0 for none, P-Age, P-Sex, P-Sample size, P-Condition, I-Surgical, I-Physical, I-Drug, I-Educational, I-Psychological, I-Other, I-Control, O-Physical, O-Pain, O-Mortality, O-Adverse effects, O-Mental, O-Other. It is crucial that you return exactly 75 labels for each chunk, ensuring that the number of labels matches the number of tokens exactly.\"},\n",
    "            {\"role\": \"user\", \"content\": GPT_df.iloc[50][\"sentence\"][:521]}, # 75 token long 1-shot sentence\n",
    "            {\"role\": \"assistant\", \"content\": GPT_df.iloc[50][\"annotation\"][:200]}, # 75 token long 1-shot annotation\n",
    "            {\"role\": \"user\", \"content\": sentence}  # Sentence to annotate\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    gpt_annotations = completion.choices[0].message.content.split('•')\n",
    "\n",
    "    if len(gpt_annotations) > chunk_size:\n",
    "        gpt_annotations = gpt_annotations[:chunk_size]\n",
    "    elif len(gpt_annotations) < chunk_size:\n",
    "        gpt_annotations += ['0'] * (chunk_size - len(gpt_annotations))\n",
    "    \n",
    "    return '•'.join(gpt_annotations)\n",
    "\n",
    "\n",
    "def process_annotations(gpt_df):\n",
    "    results = pd.DataFrame(columns=['sentence', 'gpt_annotation', 'gold_annotation'])\n",
    "\n",
    "    # Iteration over each element of the Dataframe (per PMID)\n",
    "    for index, row in tqdm(GPT_df.iloc[:99].iterrows(), total=99, desc=\"Processing annotations\"):\n",
    "        sentences = row['sentence'].split('•')\n",
    "        annotations = row['annotation'].split('•')\n",
    "        \n",
    "        temp_sentences = []\n",
    "        temp_gpt_annotations = []\n",
    "        temp_gold_annotations = []\n",
    "\n",
    "        # Chuncking is taking part underneath this iteration\n",
    "        for i in range(0, len(sentences), chunk_size):\n",
    "            sentence_chunk = '•'.join(sentences[i:i+chunk_size])\n",
    "            annotation_chunk = '•'.join(annotations[i:i+chunk_size])\n",
    "            \n",
    "            # Utilizing the API call to classify\n",
    "            gpt_annotation = get_annotations(sentence_chunk)\n",
    "            \n",
    "            # Storing results\n",
    "            temp_sentences.append(sentence_chunk)\n",
    "            temp_gpt_annotations.append(gpt_annotation)\n",
    "            temp_gold_annotations.append(annotation_chunk)\n",
    "\n",
    "        # Merging results with gold data, to evaluate\n",
    "        results = pd.concat([results, pd.DataFrame({\n",
    "            'sentence': ['•'.join(temp_sentences).split(\"•\")],\n",
    "            'gpt_annotation': ['•'.join(temp_gpt_annotations).split(\"•\")],\n",
    "            'gold_annotation': ['•'.join(temp_gold_annotations).split(\"•\")]\n",
    "        })], ignore_index=True)\n",
    "\n",
    "    return results\n",
    "\n",
    "results = process_annotations(GPT_df)\n",
    "results.to_csv('annotations_results.csv', index=True) # Saving the results for later evaluation\n",
    "print(results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reloading the results for later evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = pd.read_csv('annotations_results.csv', converters={'sentence': eval, 'gpt_annotation': eval, 'gold_annotation': eval})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Confusion Matrix for GPT-4o**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract-level Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "                0       1.00      1.00      1.00        99\n",
      "        I-Control       0.58      0.97      0.73        36\n",
      "           I-Drug       0.89      0.91      0.90        65\n",
      "    I-Educational       0.64      0.75      0.69        12\n",
      "          I-Other       0.17      0.67      0.27        15\n",
      "       I-Physical       0.61      0.58      0.59        19\n",
      "  I-Psychological       0.17      0.25      0.20         4\n",
      "       I-Surgical       0.50      0.69      0.58        16\n",
      "O-Adverse effects       0.68      0.90      0.78        29\n",
      "         O-Mental       0.65      0.72      0.68        18\n",
      "      O-Mortality       0.69      1.00      0.81        11\n",
      "          O-Other       0.56      0.91      0.69        53\n",
      "           O-Pain       0.56      1.00      0.71         5\n",
      "       O-Physical       0.89      0.65      0.75        79\n",
      "            P-Age       0.76      0.87      0.81        30\n",
      "      P-Condition       0.92      1.00      0.96        91\n",
      "    P-Sample size       0.86      1.00      0.92        84\n",
      "            P-Sex       0.59      1.00      0.74        20\n",
      "\n",
      "        micro avg       0.73      0.89      0.80       686\n",
      "        macro avg       0.65      0.82      0.71       686\n",
      "     weighted avg       0.79      0.89      0.82       686\n",
      "      samples avg       0.74      0.89      0.80       686\n",
      "\n",
      "Accuracy: 0.0202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:895: UserWarning: unknown class(es) ['', '\\n\\n', '(', ')', '),', ',', '.', '.,', '0 respostas en--', '0\\u200b', 'DESIGN', 'I-Dietary', 'METHODS', 'O-Adverse Effects', 'O-physical', 'OBJECTIVE', 'OBJECTIVES', 'P-Control', 'P-Mental', 'P-O-Other', 'P-Other', 'P-Parents', 'P-Physical', 'R-Sample size', 'RESULTS', 'mg'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "#Binarizing the gold labels\n",
    "gold_labels = mlb.fit_transform(results['gold_annotation'])\n",
    "predicted_labels = mlb.transform(results['gpt_annotation'])\n",
    "\n",
    "print(classification_report(gold_labels, predicted_labels, target_names=mlb.classes_))\n",
    "\n",
    "subset_accuracy = accuracy_score(gold_labels, predicted_labels)\n",
    "print(f\"Accuracy: {subset_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Token-level Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "                0       0.80      0.84      0.82     22917\n",
      "        I-Control       0.02      0.03      0.02       120\n",
      "           I-Drug       0.04      0.03      0.03       892\n",
      "    I-Educational       0.01      0.00      0.01       295\n",
      "          I-Other       0.00      0.00      0.00       120\n",
      "       I-Physical       0.03      0.01      0.01       253\n",
      "  I-Psychological       0.00      0.00      0.00        33\n",
      "       I-Surgical       0.02      0.01      0.02       201\n",
      "O-Adverse effects       0.00      0.00      0.00       144\n",
      "         O-Mental       0.00      0.00      0.00       341\n",
      "      O-Mortality       0.01      0.02      0.02        90\n",
      "          O-Other       0.02      0.02      0.02       562\n",
      "           O-Pain       0.00      0.00      0.00        25\n",
      "       O-Physical       0.04      0.01      0.01      1756\n",
      "            P-Age       0.00      0.00      0.00       100\n",
      "      P-Condition       0.03      0.04      0.03       684\n",
      "    P-Sample size       0.01      0.02      0.01       161\n",
      "            P-Sex       0.00      0.00      0.00        49\n",
      "\n",
      "        micro avg       0.68      0.67      0.67     28743\n",
      "        macro avg       0.06      0.06      0.06     28743\n",
      "     weighted avg       0.64      0.67      0.66     28743\n",
      "      samples avg       0.67      0.67      0.67     28743\n",
      "\n",
      "Accuracy: 0.6730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:895: UserWarning: unknown class(es) ['', '\\n\\n', '(', ')', '),', ',', '.', '.,', '0 respostas en--', '0\\u200b', 'DESIGN', 'I-Dietary', 'METHODS', 'O-Adverse Effects', 'O-physical', 'OBJECTIVE', 'OBJECTIVES', 'P-Control', 'P-Mental', 'P-Other', 'P-Parents', 'P-Physical', 'R-Sample size', 'mg'] will be ignored\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "gold_labels_flat = [label for sublist in results['gold_annotation'] for label in sublist]\n",
    "predicted_labels_flat = [label for sublist in results['gpt_annotation'] for label in sublist]\n",
    "\n",
    "min_length = min(len(gold_labels_flat), len(predicted_labels_flat))\n",
    "gold_labels_flat = gold_labels_flat[:min_length]\n",
    "predicted_labels_flat = predicted_labels_flat[:min_length]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "gold_labels_binarized = mlb.fit_transform([[label] for label in gold_labels_flat])\n",
    "predicted_labels_binarized = mlb.transform([[label] for label in predicted_labels_flat])\n",
    "\n",
    "print(classification_report(gold_labels_binarized, predicted_labels_binarized, target_names=mlb.classes_))\n",
    "\n",
    "token_accuracy = accuracy_score(gold_labels_binarized, predicted_labels_binarized)\n",
    "print(f\"Accuracy: {token_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Manual Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text-based Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_formatted_data(df):\n",
    "    for idx, row in df.iterrows():\n",
    "        print(\"\\nSentence\", idx+1)\n",
    "        sentence_words = row['sentence']\n",
    "        gpt_annotations = row['gpt_annotation']\n",
    "        gold_annotations = row['gold_annotation']\n",
    "        \n",
    "        for word_idx, word in enumerate(sentence_words):\n",
    "            if word_idx < len(gpt_annotations) and word_idx < len(gold_annotations):\n",
    "                g_word = gpt_annotations[word_idx].strip()\n",
    "                gold_word = gold_annotations[word_idx].strip()\n",
    "                print(f\"{word.strip()} - GPT: {g_word}, Gold: {gold_word}\")\n",
    "\n",
    "#print_formatted_data(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visual Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "def print_formatted_data_html(df, lstm_predictions, output_file='output.html'):\n",
    "    html_output = \"<html><head><title>Annotated Sentences</title></head><body>\"\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        sentence_html = f\"<h3>Sentence {idx+1}</h3><div style='display: flex; flex-wrap: wrap; gap: 20px; align-items: flex-start;'>\"\n",
    "        sentence_words = row['sentence']\n",
    "        gpt_annotations = row['gpt_annotation']\n",
    "        gold_annotations = row['gold_annotation']\n",
    "        lstm_annotations = [label for token, label in lstm_predictions[idx]]\n",
    "        \n",
    "        for word_idx, word in enumerate(sentence_words):\n",
    "            gpt_annotation = gpt_annotations[word_idx].strip() if word_idx < len(gpt_annotations) else '!'\n",
    "            lstm_annotation = lstm_annotations[word_idx].strip() if word_idx < len(lstm_annotations) else '!'\n",
    "            gold_annotation = gold_annotations[word_idx].strip() if word_idx < len(gold_annotations) else '!'\n",
    "            \n",
    "            gpt_color = \"red\" if gpt_annotation != gold_annotation else \"black\"\n",
    "            lstm_color = \"orange\" if lstm_annotation != gold_annotation else \"black\"\n",
    "            \n",
    "            word_html = f\"<div style='text-align: center;'>\"\n",
    "            word_html += f\"<div style='color: black; font-weight: bold;'>{word.strip()}</div>\"\n",
    "            word_html += f\"<div style='color: {gpt_color};'>GPT: {gpt_annotation}</div>\"\n",
    "            word_html += f\"<div style='color: {lstm_color};'>LSTM: {lstm_annotation}</div>\"\n",
    "            word_html += f\"<div style='color: black;'>Gold: {gold_annotation}</div>\"\n",
    "            word_html += \"</div>\"\n",
    "            sentence_html += word_html\n",
    "        \n",
    "        sentence_html += \"</div>\"\n",
    "        html_output += sentence_html\n",
    "    \n",
    "    html_output += \"</body></html>\"\n",
    "    \n",
    "    # Saving it to a file\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(html_output)\n",
    "    \n",
    "    # Displaing it within this jupyter notebook\n",
    "    display(HTML(html_output))\n",
    "\n",
    "print_formatted_data_html(results, decoded_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The cell below only evaluates the results of GPT-4o.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "def print_formatted_data_html(df):\n",
    "    for idx, row in df.iterrows():\n",
    "        html_output = f\"<h3>Sentence {idx+1}</h3><div style='display: flex; flex-wrap: wrap; gap: 20px; align-items: flex-start;'>\"\n",
    "        sentence_words = row['sentence']\n",
    "        gpt_annotations = row['gpt_annotation']\n",
    "        gold_annotations = row['gold_annotation']\n",
    "        \n",
    "        for word_idx, word in enumerate(sentence_words):\n",
    "            if word_idx < len(gpt_annotations) and word_idx < len(gold_annotations):\n",
    "                gpt_color = \"red\" if gpt_annotations[word_idx].strip() != gold_annotations[word_idx].strip() else \"black\"\n",
    "                word_html = f\"<div style='text-align: center;'>\"\n",
    "                word_html += f\"<div style='color: black; font-weight: bold;'>{word.strip()}</div>\"\n",
    "                word_html += f\"<div style='color: {gpt_color};'>GPT: {gpt_annotations[word_idx].strip()}</div>\"\n",
    "                word_html += f\"<div style='color: black;'>Gold: {gold_annotations[word_idx].strip()}</div>\"\n",
    "                word_html += \"</div>\"\n",
    "                html_output += word_html\n",
    "        \n",
    "        html_output += \"</div>\"\n",
    "        display(HTML(html_output))\n",
    "\n",
    "#print_formatted_data_html(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
